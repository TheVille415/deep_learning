{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intensive Retake 1point2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fCTM92-IFabog2IyTcQnGUNiW_nDHJ80",
      "authorship_tag": "ABX9TyOEdNPz6GbtpQ/Mqp78NqCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheVille415/deep_learning/blob/master/Intensive_Retake_1point2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU9rUjOHNumy"
      },
      "source": [
        "A Single Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqFOEEuzfrQF"
      },
      "source": [
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "# # Create a network with 1 linear unit\n",
        "# # input_shape = what we are looking for - total columns\n",
        "# model = keras.Sequential([\n",
        "#     layers.Dense(units=1, input_shape=[3])\n",
        "# ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsd0teVwN5MF"
      },
      "source": [
        "Deep Neural Networks:\n",
        "<br>\n",
        "activation function = some function we use for each layers output\n",
        "we will be using rectified linear unit or ReLU\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGEyod58SSya"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # the hidden ReLU layers\n",
        "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    # the linear output layer \n",
        "    layers.Dense(units=1),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwCToQmVVeAA"
      },
      "source": [
        "# Stochastic Gradient Descent: The loss function\n",
        "<br>\n",
        "A common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCCNgovMV8P-"
      },
      "source": [
        "# Stochastic Gradient Descent: The Optimizer\n",
        "<br>\n",
        "The optimizer is an algorithm that adjusts the weights to minimize the loss. Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZRZq99Wfb1I"
      },
      "source": [
        "# Learning rate and batch size:\n",
        "\n",
        "The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
        "\n",
        "**Adam** is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU67Qc1Vgn0i"
      },
      "source": [
        "Importing Kaggle Redwine Quality Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbj8jM6LhQTs"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np3HdM3OgnN_",
        "outputId": "3952bb75-6a32-4c0d-f1fd-921bc5879e01"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "ELDw7bqVhILI",
        "outputId": "2a0ad88d-8d68-4376-c1c1-e4a28cfbe387"
      },
      "source": [
        "wine_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/winequality-red.csv')\n",
        "wine_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0               7.4             0.700         0.00  ...       0.56      9.4        5\n",
              "1               7.8             0.880         0.00  ...       0.68      9.8        5\n",
              "2               7.8             0.760         0.04  ...       0.65      9.8        5\n",
              "3              11.2             0.280         0.56  ...       0.58      9.8        6\n",
              "4               7.4             0.700         0.00  ...       0.56      9.4        5\n",
              "...             ...               ...          ...  ...        ...      ...      ...\n",
              "1594            6.2             0.600         0.08  ...       0.58     10.5        5\n",
              "1595            5.9             0.550         0.10  ...       0.76     11.2        6\n",
              "1596            6.3             0.510         0.13  ...       0.75     11.0        6\n",
              "1597            5.9             0.645         0.12  ...       0.71     10.2        5\n",
              "1598            6.0             0.310         0.47  ...       0.66     11.0        6\n",
              "\n",
              "[1599 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Bykzt50hhix2",
        "outputId": "8aa174a4-b882-46a0-9aee-408a0df05c68"
      },
      "source": [
        "# creating and training the data\n",
        "df_train = wine_df.sample(frac=0.7, random_state=0)\n",
        "df_valid = wine_df.drop(df_train.index)\n",
        "display(df_train.head(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td>10.8</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.43</td>\n",
              "      <td>2.10</td>\n",
              "      <td>0.171</td>\n",
              "      <td>27.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.99820</td>\n",
              "      <td>3.17</td>\n",
              "      <td>0.76</td>\n",
              "      <td>10.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1032</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.10</td>\n",
              "      <td>0.095</td>\n",
              "      <td>5.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99854</td>\n",
              "      <td>3.36</td>\n",
              "      <td>0.53</td>\n",
              "      <td>9.6</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>9.1</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.33</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.063</td>\n",
              "      <td>13.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.99516</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.84</td>\n",
              "      <td>11.7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>10.2</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.053</td>\n",
              "      <td>5.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99820</td>\n",
              "      <td>3.17</td>\n",
              "      <td>0.42</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "1109           10.8             0.470         0.43  ...       0.76     10.8        6\n",
              "1032            8.1             0.820         0.00  ...       0.53      9.6        5\n",
              "1002            9.1             0.290         0.33  ...       0.84     11.7        7\n",
              "487            10.2             0.645         0.36  ...       0.42     10.0        6\n",
              "\n",
              "[4 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrGRYfuah6Wb"
      },
      "source": [
        "# Scaling the data [0,1]\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGeAzkcDiOdT"
      },
      "source": [
        "# Splitting features and target\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuDbdr-SiZ_m",
        "outputId": "aaa65aa1-9b49-481e-81bb-d3f11edd6a02"
      },
      "source": [
        "print(X_train.shape)\n",
        "# after printing we see we have 11 columns or 11 inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1119, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3JbVqE-VlbT"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mae\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC1Bg20QijrG",
        "outputId": "f64b8b93-a3b4-4e54-fcf2-b9b837b45365"
      },
      "source": [
        "# time to start training\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=200,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 0s 40ms/step - loss: 0.0863 - val_loss: 0.0948\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0865 - val_loss: 0.1010\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0887 - val_loss: 0.0987\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0887 - val_loss: 0.0932\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0865 - val_loss: 0.0941\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0846 - val_loss: 0.0926\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0832 - val_loss: 0.0937\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0846 - val_loss: 0.0930\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0837 - val_loss: 0.0951\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0842 - val_loss: 0.0959\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0856 - val_loss: 0.0975\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0856 - val_loss: 0.1091\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0896 - val_loss: 0.1031\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0909 - val_loss: 0.0932\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0852 - val_loss: 0.0931\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0853 - val_loss: 0.0926\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0813 - val_loss: 0.0925\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0806 - val_loss: 0.0934\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0786 - val_loss: 0.0922\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0808 - val_loss: 0.0931\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0794 - val_loss: 0.0934\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0783 - val_loss: 0.0933\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0770 - val_loss: 0.0930\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0788 - val_loss: 0.0970\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0780 - val_loss: 0.0927\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0783 - val_loss: 0.0925\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0782 - val_loss: 0.0925\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0772 - val_loss: 0.0934\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0793 - val_loss: 0.0914\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0760 - val_loss: 0.0922\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0772 - val_loss: 0.0962\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0795 - val_loss: 0.0913\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0776 - val_loss: 0.0942\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0794 - val_loss: 0.0938\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0781 - val_loss: 0.0933\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0768 - val_loss: 0.0918\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0735 - val_loss: 0.0918\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0735 - val_loss: 0.0935\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0749 - val_loss: 0.0956\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0745 - val_loss: 0.0932\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0746 - val_loss: 0.0943\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0744 - val_loss: 0.0927\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0742 - val_loss: 0.0942\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0728 - val_loss: 0.0916\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0723 - val_loss: 0.0954\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0721 - val_loss: 0.0947\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0725 - val_loss: 0.0946\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0735 - val_loss: 0.0923\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0738 - val_loss: 0.0933\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0706 - val_loss: 0.0906\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0713 - val_loss: 0.0942\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0710 - val_loss: 0.1060\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0763 - val_loss: 0.1022\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0734 - val_loss: 0.0937\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0692 - val_loss: 0.0918\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0673 - val_loss: 0.0927\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0703 - val_loss: 0.0982\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0763 - val_loss: 0.0983\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0773 - val_loss: 0.0936\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0700 - val_loss: 0.0922\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0667 - val_loss: 0.0917\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0682 - val_loss: 0.0939\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0668 - val_loss: 0.0904\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0674 - val_loss: 0.0960\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0689 - val_loss: 0.0993\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0709 - val_loss: 0.1043\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0767 - val_loss: 0.0940\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0721 - val_loss: 0.0903\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0727 - val_loss: 0.0933\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0684 - val_loss: 0.0926\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0643 - val_loss: 0.0946\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0644 - val_loss: 0.0929\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0643 - val_loss: 0.0891\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0636 - val_loss: 0.0930\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0622 - val_loss: 0.0918\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0621 - val_loss: 0.0914\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0616 - val_loss: 0.0922\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0610 - val_loss: 0.0916\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0617 - val_loss: 0.0931\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0619 - val_loss: 0.0946\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0613 - val_loss: 0.0948\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0613 - val_loss: 0.0925\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0598 - val_loss: 0.0908\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0594 - val_loss: 0.0922\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0618 - val_loss: 0.0944\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0601 - val_loss: 0.0923\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.0570 - val_loss: 0.0985\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0630 - val_loss: 0.0987\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0661 - val_loss: 0.0951\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0653 - val_loss: 0.0899\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0581 - val_loss: 0.0940\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0565 - val_loss: 0.0911\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0594 - val_loss: 0.0933\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0567 - val_loss: 0.0961\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0587 - val_loss: 0.0967\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0611 - val_loss: 0.0972\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0612 - val_loss: 0.0961\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0597 - val_loss: 0.0937\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0637 - val_loss: 0.0939\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0642 - val_loss: 0.0969\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0631 - val_loss: 0.0989\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0628 - val_loss: 0.0973\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0677 - val_loss: 0.0949\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0618 - val_loss: 0.0895\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0550 - val_loss: 0.0910\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0553 - val_loss: 0.0931\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0572 - val_loss: 0.0910\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0572 - val_loss: 0.0946\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0548 - val_loss: 0.0942\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0553 - val_loss: 0.0914\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0537 - val_loss: 0.0925\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0543 - val_loss: 0.0921\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0541 - val_loss: 0.0906\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0531 - val_loss: 0.0992\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0563 - val_loss: 0.0933\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0567 - val_loss: 0.0912\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0541 - val_loss: 0.0906\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0511 - val_loss: 0.0973\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0541 - val_loss: 0.0934\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0515 - val_loss: 0.0913\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0526 - val_loss: 0.0914\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0498 - val_loss: 0.0914\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0524 - val_loss: 0.0935\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0535 - val_loss: 0.0905\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0505 - val_loss: 0.0943\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0513 - val_loss: 0.0912\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0508 - val_loss: 0.0916\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0488 - val_loss: 0.0941\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0509 - val_loss: 0.0934\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0511 - val_loss: 0.0929\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0501 - val_loss: 0.0920\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0489 - val_loss: 0.0940\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0502 - val_loss: 0.0925\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0496 - val_loss: 0.0929\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0471 - val_loss: 0.0916\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0479 - val_loss: 0.0943\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0493 - val_loss: 0.0945\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0499 - val_loss: 0.0920\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0473 - val_loss: 0.0926\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0491 - val_loss: 0.0914\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0473 - val_loss: 0.0922\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0505 - val_loss: 0.0942\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0464 - val_loss: 0.0968\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0474 - val_loss: 0.0924\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0480 - val_loss: 0.0922\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0452 - val_loss: 0.0926\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0461 - val_loss: 0.0946\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0482 - val_loss: 0.0951\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0457 - val_loss: 0.0931\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0446 - val_loss: 0.0926\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0474 - val_loss: 0.0974\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0473 - val_loss: 0.0950\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0471 - val_loss: 0.0945\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0452 - val_loss: 0.0917\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 34ms/step - loss: 0.0467 - val_loss: 0.0962\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0512 - val_loss: 0.0983\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0495 - val_loss: 0.0941\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0473 - val_loss: 0.0984\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0521 - val_loss: 0.0976\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0495 - val_loss: 0.0943\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0459 - val_loss: 0.0966\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0488 - val_loss: 0.0956\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0462 - val_loss: 0.0976\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0445 - val_loss: 0.0936\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0418 - val_loss: 0.0928\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0423 - val_loss: 0.0928\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0446 - val_loss: 0.0968\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0463 - val_loss: 0.0986\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0524 - val_loss: 0.0964\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0462 - val_loss: 0.0936\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.0433 - val_loss: 0.0949\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0441 - val_loss: 0.0942\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0404 - val_loss: 0.0949\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0407 - val_loss: 0.0918\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0404 - val_loss: 0.0936\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0408 - val_loss: 0.0946\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0419 - val_loss: 0.0939\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0420 - val_loss: 0.0948\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0449 - val_loss: 0.0971\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0429 - val_loss: 0.0980\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0442 - val_loss: 0.0935\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0392 - val_loss: 0.0943\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0398 - val_loss: 0.0963\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0448 - val_loss: 0.0956\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0436 - val_loss: 0.0949\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0452 - val_loss: 0.0940\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0398 - val_loss: 0.0950\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0401 - val_loss: 0.0930\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0388 - val_loss: 0.0961\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 33ms/step - loss: 0.0396 - val_loss: 0.0934\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0379 - val_loss: 0.0942\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0417 - val_loss: 0.0968\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0416 - val_loss: 0.0957\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0400 - val_loss: 0.0938\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 32ms/step - loss: 0.0432 - val_loss: 0.1012\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 31ms/step - loss: 0.0493 - val_loss: 0.1029\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0506 - val_loss: 0.0948\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0470 - val_loss: 0.0937\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0423 - val_loss: 0.0944\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0408 - val_loss: 0.0954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "MAZ9L7E1pTeJ",
        "outputId": "fdd65c3d-ac84-408f-b549-ac2fbdfed732"
      },
      "source": [
        "# convert the training history to a dataframe\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[0:, ['loss']].plot();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}